{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "569d9c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509126/2763989798.py:37: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert N == 16_000, \"Input must be exactly 16 000 samples\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Exported to /media/dadatron/squirrel/notebooks/zk/small_audio_transformer_static.onnx\n"
     ]
    }
   ],
   "source": [
    "# ── 0. Setup ──────────────────────────────────────────────────────────────\n",
    "# pip install --upgrade torch onnx onnxruntime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "# ── 1. Model definition (unchanged) ───────────────────────────────────────\n",
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size: int = 400,\n",
    "        embed_dim: int = 32,\n",
    "        num_layers: int = 8,\n",
    "        num_heads: int = 8,\n",
    "        mlp_dim: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        num_patches = 16_000 // patch_size\n",
    "        self.patch_embed = nn.Linear(patch_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n",
    "        self.head = nn.Linear(embed_dim, 4)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N = x.shape\n",
    "        assert N == 16_000, \"Input must be exactly 16 000 samples\"\n",
    "        x = x.view(B, -1, self.patch_size)\n",
    "        x = self.patch_embed(x) + self.pos_embed\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = torch.sigmoid(self.head(x)) * 10\n",
    "        return x\n",
    "\n",
    "# ── 2. Instantiate & dummy input ──────────────────────────────────────────\n",
    "model = AudioTransformer()\n",
    "model.eval()\n",
    "\n",
    "dummy = torch.randn(1, 16_000)  # (batch=1, samples)\n",
    "\n",
    "# ── 3. Export to ONNX ─────────────────────────────────────────────────────\n",
    "onnx_path = Path(\"small_audio_transformer_static.onnx\")\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy,\n",
    "    onnx_path.as_posix(),\n",
    "    export_params=True,\n",
    "    opset_version=17,           # >=17 recommended for latest runtimes\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"waveform\"],\n",
    "    output_names=[\"scores\"],\n",
    "\n",
    ")\n",
    "print(f\"✔️  Exported to {onnx_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dc246be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20612\n"
     ]
    }
   ],
   "source": [
    "import onnx, math\n",
    "print(sum(math.prod(t.dims) for t in onnx.load(\"small_audio_transformer_static.onnx\").graph.initializer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05eb5713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX output: [[5.570454  6.7199993 3.6120117 5.174857 ]] (1, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ── 4. Quick correctness check ────────────────────────────────────────────\n",
    "ort_sess = ort.InferenceSession(onnx_path.as_posix(), providers=[\"CPUExecutionProvider\"])\n",
    "out   = ort_sess.run(None, {\"waveform\": dummy.numpy()})[0]\n",
    "print(\"ONNX output:\", out, out.shape)   # (1, 4) in [0,10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26978b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(1, 16_000)  # (batch=1, samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b412de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dynamic-INT8 model saved to small_audio_transformer_static_int8_dyn.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "float_model = \"small_audio_transformer_static.onnx\"\n",
    "int8_model  = \"small_audio_transformer_static_int8_dyn.onnx\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=float_model,\n",
    "    model_output=int8_model,\n",
    "    weight_type=QuantType.QInt8,   # signed 8-bit weights\n",
    "    # per_channel=True,            # optional: finer granularity\n",
    "    # reduce_range=True,           # optional: 7-bit weights\n",
    ")\n",
    "print(\"✅ dynamic-INT8 model saved to\", int8_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "151b02f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer dtypes: {'FP32': 18, 'INT8': 12}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import TensorProto\n",
    "\n",
    "model = onnx.load(\"small_audio_transformer_static_int8_dyn.onnx\")\n",
    "\n",
    "dtype_map = {\n",
    "    TensorProto.INT8:  \"INT8\",\n",
    "    TensorProto.UINT8: \"UINT8\",\n",
    "    TensorProto.FLOAT: \"FP32\",\n",
    "    TensorProto.FLOAT16: \"FP16\",\n",
    "    # add others if needed\n",
    "}\n",
    "\n",
    "counts = {}\n",
    "for init in model.graph.initializer:\n",
    "    dt = dtype_map.get(init.data_type, str(init.data_type))\n",
    "    counts[dt] = counts.get(dt, 0) + 1\n",
    "\n",
    "print(\"Initializer dtypes:\", counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1f4b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26768\n"
     ]
    }
   ],
   "source": [
    "import onnx, math\n",
    "print(sum(math.prod(t.dims) for t in onnx.load(\"small_audio_transformer_static_int8_dyn.onnx\").graph.initializer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c1378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9db49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sad310",
   "language": "python",
   "name": "sad310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
